# ring-attention
Optimized kernels for ring-attention [WIP]


### Weekly ring-attn Meeting

Every Sunday 5 UTC we meet in the "General" voice channel of the CUDA MODE discord server. Contact us any time asynchronously in the `#ring-attention` channel.


### Reserach / Material

- [Ring Attention with Blockwise Transformers for Near-Infinite Context](https://arxiv.org/abs/2310.01889), code: [lhao499/ring-attention](https://github.com/lhao499/ring-attention)
- [World Model on Million-Length Video And Language With RingAttention](https://arxiv.org/abs/2402.08268), code: [LargeWorldModel/LWM](https://github.com/LargeWorldModel/LWM), project site: [largeworldmodel.github.io](https://largeworldmodel.github.io/), models: [HF/LargeWorldModel](https://huggingface.co/LargeWorldModel)

- NVIDIA Online-Softmax paper (2018): [Online normalizer calculation for softmax](https://arxiv.org/abs/1805.02867)
- [ELI5: FlashAttention](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad) by [Aleksa GordiÄ‡](https://twitter.com/gordic_aleksa)
- LWM model in ollama: https://ollama.com/ifioravanti/lwm
- lucid rains pytorch impl (wip?): [lucidrains/ring-attention-pytorch](https://github.com/lucidrains/ring-attention-pytorch)



### Notebooks
- [Incremental Softmax](https://colab.research.google.com/drive/1PNDTLx2UYYk8XmTb9e_ZBxPx8P6eByvx?usp=sharing)
- [Naive flash-attn](https://colab.research.google.com/drive/1X-x6PCRydNY9LZBPLA0DZh3Tj2Dyz60M?usp=sharing)


## How to contribute

Contact us on the **CUDA MODE** discord server: [https://discord.gg/cudamode](https://discord.gg/cudamode), PRs are welcome (please create an issue first).
